{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitting-people",
   "metadata": {},
   "source": [
    "<!--\n",
    "---\n",
    "title: Extract zonal statistics from many files in parallel\n",
    "layout: single\n",
    "author: Heather Savoy\n",
    "author_profile: true\n",
    "header:\n",
    "  overlay_color: \"444444\"\n",
    "  overlay_image: /assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg\n",
    "---\n",
    "-->\n",
    "\n",
    "**Last Update:** 28 September 2022 <br />\n",
    "**Download Jupyter Notebook**: [GRWG22_ZonalStats_wSLURM.ipynb](https://geospatial.101workbook.org/tutorials/GRWG22_ZonalStats_wSLURM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-albany",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This tutorial covers how to:\n",
    "\n",
    "1. calculate zonal statistics (i.e., extract summaries of raster values \n",
    "intersecting polygons) in python, and \n",
    "2. use SLURM job arrays to execute an Python script with different inputs across \n",
    "multiple cores.\n",
    "\n",
    "We will use 21 years of the PRISM gridded dataset's annual precipitation variable \n",
    "and the US Census counties polygon dataset to calculate the mean annual precipitation \n",
    "in each county per year. We will request SLURM to distribute the 21 years of input data \n",
    "across as many cores and run our zonal statistics Python script on each one.\n",
    "\n",
    "If you prefer to have a python script handle looping over your data inputs and \n",
    "submitting many job submission scripts, see [this tutorial](https://geospatial.101workbook.org/ExampleGeoWorkflows/GRWG22_JobPerDataFile_python).\n",
    "\n",
    "*Language*: `Python`\n",
    "\n",
    "*Primary Libraries/Packages*:\n",
    "\n",
    "|Name|Description|Link|\n",
    "|-|-|-|\n",
    "| `geopandas` | Extends datatypes used by pandas to allow spatial operations on geometric types | https://geopandas.org/en/stable/ |\n",
    "| `rasterstats` | Summarizes geospatial raster datasets based on vector geometries | https://pythonhosted.org/rasterstats/ |\n",
    "\n",
    "\n",
    "## Nomenclature\n",
    "\n",
    "* *GeoCDL:* Geospatial Common Data Library, \n",
    "a collection of commonly used raster datasets accessible from an API running \n",
    "on SCINet's Ceres cluster\n",
    "* *SLURM Workload Manager:* The software on Ceres and Atlas that allocates \n",
    "compute cores to users for their submitted jobs. \n",
    "* *Zonal statistics:* Calculating summary statistics, e.g. mean, of cell values\n",
    "from a raster in each region, where regions can be defined by an overlapping \n",
    "polygon feature collection.\n",
    "\n",
    "## Data Details\n",
    "\n",
    "* Data: US Census Cartographic Boundary Files\n",
    "* Link: https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\n",
    "* Other Details: The cartographic boundary files are simplified representations \n",
    "  of selected geographic areas from the Census Bureauâ€™s MAF/TIGER geographic \n",
    "  database. These boundary files are specifically designed for small scale \n",
    "  thematic mapping. \n",
    "  \n",
    "* Data: PRISM\n",
    "* Link: https://prism.oregonstate.edu/\n",
    "* Other Details: The PRISM Climate Group gathers climate observations from a \n",
    "  wide range of monitoring networks, applies sophisticated quality control measures, \n",
    "  and develops spatial climate datasets to reveal short- and long-term climate \n",
    "  patterns. The resulting datasets incorporate a variety of modeling techniques \n",
    "  and are available at multiple spatial/temporal resolutions, covering the period \n",
    "  from 1895 to the present.\n",
    "\n",
    "## Analysis Steps\n",
    "\n",
    "* Write serial python script - this script will accept a year argument, open the \n",
    "  raster file associated with that year, open the polygon dataset, calculate \n",
    "  the mean value per polygon, and write a new shapefile with the mean values.\n",
    "* Write and save a SLURM job submission script - Create a batch script with\n",
    "  SLURM commands requesting resources to execute your calculations on multiple\n",
    "  cores.\n",
    "* Submit your job - Submit your batch script to SLURM\n",
    "* Check results - Monitor the SLURM queue until your job is complete and then \n",
    "  ensure your job executed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bade1",
   "metadata": {},
   "source": [
    "### Step 0: Install packages and download data\n",
    "\n",
    "Below are commands to run to create a new Conda environment named 'geoenv' that contains the packages used in this tutorial series. To learn more about using Conda environments on Ceres, see [this guide](https://scinet.usda.gov/guide/conda/). NOTE: If you have used other Geospatial Workbook tutorials from the SCINet Geospatial Research Working Group Workshop 2022, you may have aleady created this environment and may skip to activating the environment, opening python, and downloading data.\n",
    "\n",
    "First, we call `salloc` to be allocated resources on a compute node so we do not burden the login node with the conda installations. Then we load the `miniconda` conda module available on Ceres to access the `Conda` commands to create environments, activate them, and install Python and packages. Last, we open the version of python in the environment for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7215d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "salloc\n",
    "module load miniconda\n",
    "conda create --name geoenv\n",
    "source activate geoenv\n",
    "conda install geopandas rioxarray rasterstats plotnine ipython dask dask-jobqueue -c conda-forge\n",
    "python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d896af7",
   "metadata": {},
   "source": [
    "The code chunk below will download the example data. For our US county polygons, \n",
    "we are downloading a zipped folder containing a shapefile. For our precipitation data, we are using the SCINet GeoCDL\n",
    "to download annual precipitation for 2000-2020 in a bounding box \n",
    "approximately covering the state of Florida. Feel free to change the latitude\n",
    "and longitude to your preferred area, but any mentions of processing times below \n",
    "will reflect the bounds provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Vector data\n",
    "vector_base = 'us_counties2021'\n",
    "vector_zip = vector_base + '.zip'\n",
    "urllib.request.urlretrieve('https://www2.census.gov/geo/tiger/GENZ2021/shp/cb_2021_us_county_20m.zip', vector_zip)\n",
    "with zipfile.ZipFile(vector_zip,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(vector_base)\n",
    "\n",
    "# Raster data\n",
    "geocdl_url = 'http://10.1.1.80:8000/subset_polygon?datasets=PRISM%3Appt&years=2000:2020&clip=(-87.5,31),(-79,24.5)'\n",
    "raster_base = 'ppt_for_zonal_stats'\n",
    "raster_zip = raster_base + '.zip'\n",
    "urllib.request.urlretrieve(geocdl_url, raster_zip)\n",
    "with zipfile.ZipFile(raster_zip,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(raster_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfaa8dd",
   "metadata": {},
   "source": [
    "You may now exit python by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-orchestra",
   "metadata": {},
   "source": [
    "### Step 1: Write and save a serial python script that accepts command line arguments\n",
    "\n",
    "Save these lines below as `zonal_stats.py` on Ceres. It is a python script that:\n",
    "\n",
    "1. Takes one argument from the command line to specify the data input year\n",
    "2. Points to the raster data file associated with that year\n",
    "3. Opens the vector data\n",
    "4. Extracts the mean values per polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-external",
   "metadata": {},
   "source": [
    "```python\n",
    "import argparse\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "from shapely.geometry import box\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# Read in command line arguments. Expecting a year value.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"year\",type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Read in the raster corresponding to the year argument\n",
    "r_fname = 'ppt_for_zonal_stats/PRISM_ppt_' + str(args.year) + '.tif'\n",
    "my_raster = rioxarray.open_rasterio(r_fname)\n",
    "raster_bounds = my_raster.rio.bounds()\n",
    "\n",
    "# Read in the polygon shapefile and transform it to match raster\n",
    "my_polygons = gpd.read_file('us_counties2021/cb_2021_us_county_20m.shp')\n",
    "rbounds_df = gpd.GeoDataFrame({\"id\":1,\"geometry\":[box(*raster_bounds)]},\n",
    "                              crs = my_raster.rio.crs)\n",
    "my_polygons = my_polygons.to_crs(my_raster.rio.crs).clip(rbounds_df)\n",
    "\n",
    "# Extract mean raster value per polygon\n",
    "zs_gj = zonal_stats(\n",
    "    my_polygons, \n",
    "    r_fname, \n",
    "    stats = \"mean\",\n",
    "    geojson_out = True)\n",
    "zs_gpd = gpd.GeoDataFrame.from_features(zs_gj)\n",
    "\n",
    "# Save extracted mean values in a shapefile\n",
    "zs_gpd.to_file('stats_' + str(args.year) + '.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-broadway",
   "metadata": {},
   "source": [
    "<a id='step_2'></a>\n",
    "### Step 2: Write and save a SLURM job submission script\n",
    "\n",
    "Now that we have our python script that accepts a year as input, we will write a \n",
    "SLURM job batch script to request that python script be called over an array of years. \n",
    "This kind of job submission is known as a 'job array'. Each 'task' in the job \n",
    "array, each year in our case, will be treated like its own job in the SLURM queue,\n",
    "but with the benefit of only having to submit one submission batch script. \n",
    "\n",
    "Save the lines below as `zonal_stats.sh`. The lines starting with `#SBATCH` are\n",
    "instructions for SLURM about how long your job will take to run, how many cores\n",
    "you need, etc. The lines that follow afterwards are like any other batch script.\n",
    "Here, we are loading required modules and then executing our python script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-parameter",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --time=00:30:00       # walltime limit (HH:MM:SS) \n",
    "#SBATCH --nodes=1             # number of nodes\n",
    "#SBATCH --ntasks-per-node=2   # 1 processor core(s) per node X 2 threads per core\n",
    "#SBATCH --partition=short     # standard node(s)\n",
    "#SBATCH --array=2000-2020     # your script input (the years)\n",
    "#SBATCH --output=slurm_%A_%a.out  # format of output filename\n",
    "\n",
    "# LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE\n",
    "module load miniconda\n",
    "source activate geoenv\n",
    "\n",
    "python zonal_stats.py ${SLURM_ARRAY_TASK_ID}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce58f6",
   "metadata": {},
   "source": [
    "The meaning of our parameter choices:\n",
    "\n",
    "* `time=00:30:00`: Our tasks will take up to 30 minutes to run. \n",
    "* `nodes=1`: We only need one node. If you are just getting started with parallel\n",
    "processing, you will likely only need one node. \n",
    "* `ntasks-per-node=2`: We want two logical cores on our one node, i.e. each task\n",
    "will use one physical core. Our individual tasks are serial and only need one core. \n",
    "* `partition=short`: We will use the 'short' partition on Ceres, the collection \n",
    "of nodes dedicated to shorter walltime jobs not requiring extensive memory. See\n",
    "[this guide](https://scinet.usda.gov/guide/ceres/#partitions-or-queues) for more \n",
    "information about the available partitions on Ceres. \n",
    "* `array=2000-2020`: This is the parameter that tells SLURM we want a job array:\n",
    "although we are submitting one job script, treat it as an array of many tasks. \n",
    "Those tasks should have IDs in the range of 2000-2020 to represent the years of\n",
    "data we want analyzed.\n",
    "* `--output=slurm_%A_%a.out`: Save any output from python (e.g. printed messages,\n",
    "warnings, and errors) to a file with a filename in the format of \n",
    "*output_JOBID_TASKID.out*. The *JOBID* is assigned when the job is submitted (see\n",
    "the next step) and the *TASKID* will be in the range of our tasks in the array, \n",
    "2000-2020. This way, if a particular year runs into an error, it can be easily\n",
    "found. \n",
    "\n",
    "Note: there are additional SLURM parameters you may use, including how to specify\n",
    "your own job ID or setting memory requirements. Check out the \n",
    "[Ceres job script generator](https://scinet.usda.gov/support/ceres-job-script) \n",
    "to see more examples on how to populate job submission scripts on Ceres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-indicator",
   "metadata": {},
   "source": [
    "<a id='submit'></a>\n",
    "### Step 3: Submit your job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38522168",
   "metadata": {},
   "source": [
    "Now that we have our packages, data, python script, and job submission script prepared,\n",
    "we can finally submit the job. To submit a batch job to SLURM, we use the command \n",
    "`sbatch` followed by the job submission script filename via our shell. After you \n",
    "run this line, you will see a message with your job ID. You can use this to \n",
    "identify this job in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-domain",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sbatch zonal_stats.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86a69e",
   "metadata": {},
   "source": [
    "### Step 4: Watch the queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd4fbc",
   "metadata": {},
   "source": [
    "To see the status of your job, you can view the SLURM queue. The queue lists all\n",
    "of the jobs currently submitted, who submitted them, the job status, and what\n",
    "nodes are allocated to the job. Since this can be a very long list, it is easiest\n",
    "to find your jobs if you filter the queue to only the jobs you submitted. The \n",
    "command to view the queue is `squeue` and you can filter it to a specific user\n",
    "with the `-u` parameter followed by their SCINet account name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26088642",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "squeue -u firstname.lastname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95778cd6",
   "metadata": {},
   "source": [
    "If you see jobs listed in the queue: you have jobs currently in the queue and the status \n",
    "column will indicate if that job is pending, running, or completing. \n",
    "\n",
    "If you do NOT see jobs listed in the queue: you do not have jobs currently in the\n",
    "queue. If you submitted jobs but they are not listed, then they completed - either\n",
    "successfully or unsuccessfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763e3c2",
   "metadata": {},
   "source": [
    "<a id='results'></a>\n",
    "### Step 5: Check results\n",
    "\n",
    "To determine if the job executed successfully, \n",
    "you may check if your anticipated output was created. In our case, we would expect\n",
    "to see new shapefiles of the format *stats_YYYY.shp*. If you do not see your \n",
    "anticipated results, you can read the contents of the *output_JOBID_TASKID.out*\n",
    "files to check for error messages. \n",
    "\n",
    "Here is a visual of our 2020 result in *stats_2020.shp* showing the mean 2020 \n",
    "total precipitation per county in Florida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118be561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "result = gpd.read_file('stats_1.shp')\n",
    "result.plot('mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gcdlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5857a5495813576f3cc4b710578430cd3f3841742f74b70f765edd03f07bc41c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
